{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos\n",
    "#### http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jsanch90/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Eliminacion todos los caracteres especiales o simbolos.\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_data_txt(file_path):\n",
    "    _file = open(file_path,'r')\n",
    "    data = _file.read()\n",
    "    symbols = re.compile(r'[!\"#$%&\\()*+,-./:;<=>?@\\[\\]\\\\^_`{|}~]')\n",
    "    clean_data = symbols.sub('', data)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Clase para graficar los datos de entrenamiento y validacion en una sola grafica en TensorBoard\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para leer un directorio con varios archivos archivos de comentarios y retornar 2 listas, una con los datos y otra con las etiquetas\n",
    "def dir_to_lists(files_path,label):\n",
    "    files = os.listdir(files_path)\n",
    "    data = []\n",
    "    for _file in files:\n",
    "        data.append(get_data_txt(files_path+'/'+_file))\n",
    "    labels = [label]*len(data)\n",
    "    \n",
    "    return (data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para guardar todos los datos de los archivos txt separados en un unico CSV\n",
    "def data_to_csv(comments,labels,name):\n",
    "    data = {'comments': comments, 'labels':labels}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv(name,sep=',', index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "neg_train = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/train/neg',0)\n",
    "pos_train = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/train/pos',1)\n",
    "neg_test = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/test/neg',0)\n",
    "pos_test = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/train/pos',1)\n",
    "\n",
    "\n",
    "data = pos_train[0]+neg_train[0]+pos_test[0]+neg_test[0]\n",
    "labels = pos_train[1]+neg_train[1]+pos_test[1]+neg_test[1]\n",
    "\n",
    "df = data_to_csv(data,labels,'./data_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea la funcion para crear el arreglo con los indices de las palabras que estan en el diccionario\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "def get_indices_from_review(review):\n",
    "    regex = re.compile(r'[!\"#$%&\\()*+,-./:;<=>?@\\[\\]\\\\^_`{|}~]')\n",
    "    s = regex.sub('', review)\n",
    "    # 2 is \"unknown\"\n",
    "    sequence = map(lambda word: word_index.get(word, 2) + 3, s.lower().split())\n",
    "    sequence = map(lambda index: 2 if index >= 80000 else index, sequence)\n",
    "    # 1 is \"start of sequence\"\n",
    "    return [1] + list(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el vector que retorna la funcion anterior lo pasamos a un vector de 0 y 1, cuyo tama√±o es el numero de palabras que tenemos en el diccionario\n",
    "def vectorize_sequences(sequences, dim):\n",
    "    vec = np.zeros(shape=(len(sequences), dim), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        vec[i, seq] = 1\n",
    "    return vec\n",
    "#vectorize_sequences([[1,5,2],[7,1,2],[9,5,2]],dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 18)                1440018   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,440,339\n",
      "Trainable params: 1,440,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defnicion del modelo, se utilizaron 30000 palabras del diccionario, en este modelo no se utilizo dropout\n",
    "model_no_drop = keras.Sequential([\n",
    "  keras.layers.Dense(units=18, activation='relu', input_shape=(80000,)),\n",
    "  keras.layers.Dense(units=16, activation='relu'),\n",
    "  keras.layers.Dense(units=1, activation='sigmoid')\n",
    "], name='comments_review')\n",
    "model_no_drop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminacion de palabras de parada de un comentario\n",
    "stop_words = ['a','about','above','after','again','all','an','and','any','as','at','be','because','been','before','being','below','between','both','but','by','down','during','each',\n",
    " 'few','for','from','further','he','her','here','hers','herself','him','himself','his','how','if','in','into','it',\"it's\",'its','itself','just','ll','m','ma','me','my','myself',\n",
    " 'now','o','of','on','once','only','or','other','our','ours','ourselves','out','over','own','re','s','same','she',\"she's\",'so','such','t','than','that',\"that'll\",'the','their','theirs',\n",
    " 'them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','ve','what','when','where','which','while','who','whom','why','will',\n",
    " 'with','won','y','you',\"you'd\",\"you'll\",\"you're\",\"you've\",'your','yours','yourself','yourselves']\n",
    "\n",
    "def clean_data(str):\n",
    "    wordsFiltered = []\n",
    "    words = nltk.word_tokenize(str)\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w)!=1:\n",
    "            wordsFiltered.append(w)\n",
    "    return \" \".join(wordsFiltered)\n",
    "\n",
    "data_stop = pd.read_csv('./data_complete.csv')\n",
    "\n",
    "data_stop['comments_clean'] = data_stop['comments'].apply(clean_data)\n",
    "\n",
    "x_stop = data_stop['comments_clean']\n",
    "y_stop = data_stop['labels']\n",
    "\n",
    "\n",
    "\n",
    "x_index_stop = []\n",
    "for i in x_stop:\n",
    "    x_index_stop.append(get_indices_from_review(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionamiento de los datos que ni tienen palabras de parada\n",
    "x_stop_ = vectorize_sequences(x_index_stop,dim=80000)\n",
    "y_stop_ = np.expand_dims(np.asarray(y_stop, dtype=np.float32), axis=-1)\n",
    "x_train_stop,x_val_test_stop,y_train_stop, y_val_test_stop = train_test_split(x_stop_,y_stop_,test_size=0.3,shuffle=True)\n",
    "x_val_stop,x_test_stop,y_val_stop, y_test_stop = train_test_split(x_val_test_stop,y_val_test_stop,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_complete.csv')\n",
    "x = data['comments']\n",
    "y = data['labels']\n",
    "\n",
    "\n",
    "x_index = []\n",
    "for i in x:\n",
    "    x_index.append(get_indices_from_review(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particinamiento de los datos con palabras de parada\n",
    "x_ = vectorize_sequences(x_index,dim=80000)\n",
    "y_ = np.expand_dims(np.asarray(y, dtype=np.float32), axis=-1)\n",
    "x_train,x_val_test,y_train, y_val_test = train_test_split(x_,y_,test_size=0.3,shuffle=True)\n",
    "x_val,x_test,y_val, y_test = train_test_split(x_val_test,y_val_test,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 10500 samples\n",
      "Epoch 1/20\n",
      "35000/35000 [==============================] - 47s 1ms/step - loss: 0.2840 - acc: 0.8902 - val_loss: 0.2151 - val_acc: 0.9201\n",
      "Epoch 2/20\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0934 - acc: 0.9682 - val_loss: 0.2219 - val_acc: 0.9298\n",
      "Epoch 3/20\n",
      "35000/35000 [==============================] - 43s 1ms/step - loss: 0.0314 - acc: 0.9903 - val_loss: 0.3033 - val_acc: 0.9280\n",
      "Epoch 4/20\n",
      "35000/35000 [==============================] - 43s 1ms/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.3393 - val_acc: 0.9309\n",
      "Epoch 5/20\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.5662 - val_acc: 0.9074\n",
      "Epoch 6/20\n",
      "35000/35000 [==============================] - 39s 1ms/step - loss: 0.0097 - acc: 0.9970 - val_loss: 0.4763 - val_acc: 0.9216\n",
      "Epoch 7/20\n",
      "35000/35000 [==============================] - 40s 1ms/step - loss: 0.0071 - acc: 0.9976 - val_loss: 0.4704 - val_acc: 0.9297\n",
      "Epoch 8/20\n",
      "35000/35000 [==============================] - 41s 1ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.5709 - val_acc: 0.9170\n",
      "Epoch 9/20\n",
      "35000/35000 [==============================] - 40s 1ms/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.5192 - val_acc: 0.9246\n",
      "Epoch 10/20\n",
      "35000/35000 [==============================] - 43s 1ms/step - loss: 4.9430e-04 - acc: 0.9999 - val_loss: 0.5514 - val_acc: 0.9304\n",
      "Epoch 11/20\n",
      "35000/35000 [==============================] - 41s 1ms/step - loss: 1.3481e-04 - acc: 1.0000 - val_loss: 0.5793 - val_acc: 0.9302\n",
      "Epoch 12/20\n",
      "35000/35000 [==============================] - 41s 1ms/step - loss: 7.5295e-05 - acc: 1.0000 - val_loss: 0.6117 - val_acc: 0.9276\n",
      "Epoch 13/20\n",
      "35000/35000 [==============================] - 39s 1ms/step - loss: 5.1323e-05 - acc: 1.0000 - val_loss: 0.6260 - val_acc: 0.9282\n",
      "Epoch 14/20\n",
      "35000/35000 [==============================] - 41s 1ms/step - loss: 4.0282e-05 - acc: 1.0000 - val_loss: 0.6373 - val_acc: 0.9282\n",
      "Epoch 15/20\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 2.5328e-05 - acc: 1.0000 - val_loss: 0.6661 - val_acc: 0.9284\n",
      "Epoch 16/20\n",
      "35000/35000 [==============================] - 40s 1ms/step - loss: 1.2762e-05 - acc: 1.0000 - val_loss: 0.6907 - val_acc: 0.9278\n",
      "Epoch 17/20\n",
      "35000/35000 [==============================] - 40s 1ms/step - loss: 4.3696e-06 - acc: 1.0000 - val_loss: 0.7136 - val_acc: 0.9276\n",
      "Epoch 18/20\n",
      "35000/35000 [==============================] - 41s 1ms/step - loss: 1.1126e-06 - acc: 1.0000 - val_loss: 0.7176 - val_acc: 0.9286\n",
      "Epoch 19/20\n",
      "35000/35000 [==============================] - 40s 1ms/step - loss: 6.1601e-07 - acc: 1.0000 - val_loss: 0.7356 - val_acc: 0.9286\n",
      "Epoch 20/20\n",
      "35000/35000 [==============================] - 41s 1ms/step - loss: 3.7828e-07 - acc: 1.0000 - val_loss: 0.7380 - val_acc: 0.9290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feadd2b58d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenamiento del modelo sin dropout\n",
    "model_no_drop.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_no_drop.fit(x_train_stop, y_train_stop,\n",
    "          epochs=20, batch_size=32,\n",
    "          validation_data=(x_val_stop, y_val_stop),\n",
    "          callbacks=[keras.callbacks.TensorBoard(),TrainValTensorBoard(write_graph=False,log_dir='logs_comments_review_2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500/4500 [==============================] - 2s 510us/step\n",
      "Test loss: 0.250258297643\n",
      "Test accuracy: 0.968444444444\n"
     ]
    }
   ],
   "source": [
    "#Evaluacion del modelo con el conjunto de pruebas\n",
    "test_loss, test_acc = model_no_drop.evaluate(x=x_test, y=y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_drop.save('./model_no_drop_9684.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "test: 2.5167%\n"
     ]
    }
   ],
   "source": [
    "# prueba del modelo con un comentario desconocido por el modelo\n",
    "review = \"\"\"\n",
    "Choreography is all that's on offer here. The increasingly convoluted fight scenes turn into lifeless spectacle in this ugly franchise.\"\"\"\n",
    "review_vec = get_indices_from_review(review)\n",
    "vec = vectorize_sequences([review_vec], dim=80000)\n",
    "print(vec)\n",
    "res = np.squeeze(model_no_drop.predict(vec))\n",
    "print('test: {:.4f}%'.format(res * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 18)                1440018   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,440,339\n",
      "Trainable params: 1,440,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# definicion del modelo con dropout para evitar el sobreajuste\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Dense(units=18, activation='relu', input_shape=(80000,)),\n",
    "  keras.layers.Dropout(0.7),\n",
    "  keras.layers.Dense(units=16, activation='relu'),\n",
    "  keras.layers.Dropout(0.7),\n",
    "  keras.layers.Dense(units=1, activation='sigmoid')\n",
    "], name='with_dropout')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 10500 samples\n",
      "Epoch 1/20\n",
      "35000/35000 [==============================] - 54s 2ms/step - loss: 0.6112 - acc: 0.6505 - val_loss: 0.4443 - val_acc: 0.8872\n",
      "Epoch 2/20\n",
      "35000/35000 [==============================] - 46s 1ms/step - loss: 0.4740 - acc: 0.7841 - val_loss: 0.3044 - val_acc: 0.9019\n",
      "Epoch 3/20\n",
      "35000/35000 [==============================] - 51s 1ms/step - loss: 0.3901 - acc: 0.8342 - val_loss: 0.2569 - val_acc: 0.9060\n",
      "Epoch 4/20\n",
      "35000/35000 [==============================] - 55s 2ms/step - loss: 0.3374 - acc: 0.8580 - val_loss: 0.2244 - val_acc: 0.9175\n",
      "Epoch 5/20\n",
      "35000/35000 [==============================] - 39s 1ms/step - loss: 0.3005 - acc: 0.8735 - val_loss: 0.2129 - val_acc: 0.9214\n",
      "Epoch 6/20\n",
      "35000/35000 [==============================] - 46s 1ms/step - loss: 0.2730 - acc: 0.8837 - val_loss: 0.2087 - val_acc: 0.9232\n",
      "Epoch 7/20\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.2613 - acc: 0.8861 - val_loss: 0.2121 - val_acc: 0.9235\n",
      "Epoch 8/20\n",
      "35000/35000 [==============================] - 46s 1ms/step - loss: 0.2409 - acc: 0.8976 - val_loss: 0.2112 - val_acc: 0.9269\n",
      "Epoch 9/20\n",
      "35000/35000 [==============================] - 45s 1ms/step - loss: 0.2254 - acc: 0.9002 - val_loss: 0.2189 - val_acc: 0.9275\n",
      "Epoch 10/20\n",
      "35000/35000 [==============================] - 52s 1ms/step - loss: 0.2154 - acc: 0.9003 - val_loss: 0.2185 - val_acc: 0.9279\n",
      "Epoch 11/20\n",
      "35000/35000 [==============================] - 50s 1ms/step - loss: 0.2064 - acc: 0.9051 - val_loss: 0.2240 - val_acc: 0.9294\n",
      "Epoch 12/20\n",
      "35000/35000 [==============================] - 49s 1ms/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.2334 - val_acc: 0.9301\n",
      "Epoch 13/20\n",
      "35000/35000 [==============================] - 53s 2ms/step - loss: 0.1922 - acc: 0.9103 - val_loss: 0.2499 - val_acc: 0.9287\n",
      "Epoch 14/20\n",
      "35000/35000 [==============================] - 53s 2ms/step - loss: 0.1901 - acc: 0.9094 - val_loss: 0.2372 - val_acc: 0.9310\n",
      "Epoch 15/20\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.1862 - acc: 0.9119 - val_loss: 0.2627 - val_acc: 0.9310\n",
      "Epoch 16/20\n",
      "35000/35000 [==============================] - 46s 1ms/step - loss: 0.1794 - acc: 0.9137 - val_loss: 0.2771 - val_acc: 0.9317\n",
      "Epoch 17/20\n",
      "35000/35000 [==============================] - 51s 1ms/step - loss: 0.1766 - acc: 0.9145 - val_loss: 0.2743 - val_acc: 0.9325\n",
      "Epoch 18/20\n",
      "35000/35000 [==============================] - 48s 1ms/step - loss: 0.1758 - acc: 0.9128 - val_loss: 0.2902 - val_acc: 0.9319\n",
      "Epoch 19/20\n",
      "35000/35000 [==============================] - 52s 1ms/step - loss: 0.1698 - acc: 0.9177 - val_loss: 0.3052 - val_acc: 0.9292\n",
      "Epoch 20/20\n",
      "35000/35000 [==============================] - 57s 2ms/step - loss: 0.1700 - acc: 0.9164 - val_loss: 0.2962 - val_acc: 0.9332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd790190c50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenamiento del modelo con dropout\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train_stop, y_train_stop,\n",
    "          epochs=20, batch_size=512,\n",
    "          validation_data=(x_val_stop, y_val_stop),\n",
    "          callbacks=[keras.callbacks.TensorBoard(log_dir='logs_dropout'),TrainValTensorBoard(write_graph=False,log_dir='logs_dropout_2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500/4500 [==============================] - 3s 656us/step\n",
      "Test loss: 0.273286107903\n",
      "Test accuracy: 0.935111111217\n"
     ]
    }
   ],
   "source": [
    "# evaluacion del modelo con el conjunto de pruebas\n",
    "test_loss, test_acc = model.evaluate(x=x_test_stop, y=y_test_stop)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0. ...,  0.  0.  0.]]\n",
      "test: 1.7054%\n"
     ]
    }
   ],
   "source": [
    "# prueba del modelo con un comentario desconocido por el modelo\n",
    "review = \"\"\"\n",
    "Choreography is all that's on offer here. The increasingly convoluted fight scenes turn into lifeless spectacle in this ugly franchise.\"\"\"\n",
    "review_vec = get_indices_from_review(review)\n",
    "vec = vectorize_sequences([review_vec], dim=80000)\n",
    "print(vec)\n",
    "res = np.squeeze(model.predict(vec))\n",
    "print('test: {:.4f}%'.format(res * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_drop_9351.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
