{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de los datos\n",
    "#### http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jsanch90/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Eliminacion todos los caracteres especiales o simbolos.\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_data_txt(file_path):\n",
    "    _file = open(file_path,'r')\n",
    "    data = _file.read()\n",
    "    symbols = re.compile(r'[!\"#$%&\\()*+,-./:;<=>?@\\[\\]\\\\^_`{|}~]')\n",
    "    clean_data = symbols.sub('', data)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Clase para graficar los datos de entrenamiento y validacion en una sola grafica en TensorBoard\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para leer un directorio con varios archivos archivos de comentarios y retornar 2 listas, una con los datos y otra con las etiquetas\n",
    "def dir_to_lists(files_path,label):\n",
    "    files = os.listdir(files_path)\n",
    "    data = []\n",
    "    for _file in files:\n",
    "        data.append(get_data_txt(files_path+'/'+_file))\n",
    "    labels = [label]*len(data)\n",
    "    \n",
    "    return (data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion para guardar todos los datos de los archivos txt separados en un unico CSV\n",
    "def data_to_csv(comments,labels,name):\n",
    "    data = {'comments': comments, 'labels':labels}\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv(name,sep=',', index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "neg_train = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/train/neg',0)\n",
    "pos_train = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/train/pos',1)\n",
    "neg_test = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/test/neg',0)\n",
    "pos_test = dir_to_lists('/home/josh/MEGA/U_S_VII/Ingenieria_del_conocimiento/Proyecto/dataset/aclImdb/train/pos',1)\n",
    "\n",
    "\n",
    "data = pos_train[0]+neg_train[0]+pos_test[0]+neg_test[0]\n",
    "labels = pos_train[1]+neg_train[1]+pos_test[1]+neg_test[1]\n",
    "\n",
    "df = data_to_csv(data,labels,'./data_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea la funcion para crear el arreglo con los indices de las palabras que estan en el diccionario\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "def get_indices_from_review(review):\n",
    "    regex = re.compile(r'[!\"#$%&\\()*+,-./:;<=>?@\\[\\]\\\\^_`{|}~]')\n",
    "    s = regex.sub('', review)\n",
    "    # 2 is \"unknown\"\n",
    "    sequence = map(lambda word: word_index.get(word, 2) + 3, s.lower().split())\n",
    "    sequence = map(lambda index: 2 if index >= 30000 else index, sequence)\n",
    "    # 1 is \"start of sequence\"\n",
    "    return [1] + list(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el vector que retorna la funcion anterior lo pasamos a un vector de 0 y 1, cuyo tama√±o es el numero de palabras que tenemos en el diccionario\n",
    "def vectorize_sequences(sequences, dim):\n",
    "    vec = np.zeros(shape=(len(sequences), dim), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        vec[i, seq] = 1\n",
    "    return vec\n",
    "#vectorize_sequences([[1,5,2],[7,1,2],[9,5,2]],dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 18)                540018    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 540,339\n",
      "Trainable params: 540,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defnicion del modelo, se utilizaron 30000 palabras del diccionario, en este modelo no se utilizo dropout\n",
    "model_no_drop = keras.Sequential([\n",
    "  keras.layers.Dense(units=18, activation='relu', input_shape=(30000,)),\n",
    "  keras.layers.Dense(units=16, activation='relu'),\n",
    "  keras.layers.Dense(units=1, activation='sigmoid')\n",
    "], name='comments_review')\n",
    "model_no_drop.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminacion de palabras de parada de un comentario\n",
    "stop_words = ['a','about','above','after','again','all','an','and','any','as','at','be','because','been','before','being','below','between','both','but','by','down','during','each',\n",
    " 'few','for','from','further','he','her','here','hers','herself','him','himself','his','how','if','in','into','it',\"it's\",'its','itself','just','ll','m','ma','me','my','myself',\n",
    " 'now','o','of','on','once','only','or','other','our','ours','ourselves','out','over','own','re','s','same','she',\"she's\",'so','such','t','than','that',\"that'll\",'the','their','theirs',\n",
    " 'them','themselves','then','there','these','they','this','those','through','to','too','under','until','up','ve','what','when','where','which','while','who','whom','why','will',\n",
    " 'with','won','y','you',\"you'd\",\"you'll\",\"you're\",\"you've\",'your','yours','yourself','yourselves']\n",
    "\n",
    "def clean_data(str):\n",
    "    wordsFiltered = []\n",
    "    words = nltk.word_tokenize(str)\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w)!=1:\n",
    "            wordsFiltered.append(w)\n",
    "    return \" \".join(wordsFiltered)\n",
    "\n",
    "data_stop = pd.read_csv('./data_complete.csv')\n",
    "\n",
    "data_stop['comments_clean'] = data_stop['comments'].apply(clean_data)\n",
    "\n",
    "x_stop = data_stop['comments_clean']\n",
    "y_stop = data_stop['labels']\n",
    "\n",
    "\n",
    "\n",
    "x_index_stop = []\n",
    "for i in x_stop:\n",
    "    x_index_stop.append(get_indices_from_review(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionamiento de los datos que ni tienen palabras de parada\n",
    "x_stop_ = vectorize_sequences(x_index_stop,dim=30000)\n",
    "y_stop_ = np.expand_dims(np.asarray(y_stop, dtype=np.float32), axis=-1)\n",
    "x_train_stop,x_val_test_stop,y_train_stop, y_val_test_stop = train_test_split(x_stop_,y_stop_,test_size=0.3,shuffle=True)\n",
    "x_val_stop,x_test_stop,y_val_stop, y_test_stop = train_test_split(x_val_test_stop,y_val_test_stop,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_complete.csv')\n",
    "x = data['comments']\n",
    "y = data['labels']\n",
    "\n",
    "\n",
    "x_index = []\n",
    "for i in x:\n",
    "    x_index.append(get_indices_from_review(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particinamiento de los datos con palabras de parada\n",
    "x_ = vectorize_sequences(x_index,dim=30000)\n",
    "y_ = np.expand_dims(np.asarray(y, dtype=np.float32), axis=-1)\n",
    "x_train,x_val_test,y_train, y_val_test = train_test_split(x_,y_,test_size=0.3,shuffle=True)\n",
    "x_val,x_test,y_val, y_test = train_test_split(x_val_test,y_val_test,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrenamiento del modelo sin dropout\n",
    "model_no_drop.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model_no_drop.fit(x_train, y_train,\n",
    "          epochs=20, batch_size=32,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[keras.callbacks.TensorBoard(),TrainValTensorBoard(write_graph=False,log_dir='logs_comments_review_2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluacion del modelo con el conjunto de pruebas\n",
    "test_loss, test_acc = model_no_drop.evaluate(x=x_test, y=y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba del modelo con un comentario desconocido por el modelo\n",
    "review = \"\"\"Endgame consists almost entirely of the downtime scenes that were always secretly everyone's favorite parts of these movies anyway.\"\"\"\n",
    "review_vec = get_indices_from_review(review)\n",
    "vec = vectorize_sequences([review_vec], dim=30000)\n",
    "print(vec)\n",
    "res = np.squeeze(model_no_drop.predict(vec))\n",
    "print('test: {:.4f}%'.format(res * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 18)                540018    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                304       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 540,339\n",
      "Trainable params: 540,339\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# definicion del modelo con dropout para evitar el sobreajuste\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Dense(units=18, activation='relu', input_shape=(30000,)),\n",
    "  keras.layers.Dropout(0.7),\n",
    "  keras.layers.Dense(units=16, activation='relu'),\n",
    "  keras.layers.Dropout(0.7),\n",
    "  keras.layers.Dense(units=1, activation='sigmoid')\n",
    "], name='with_dropout')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 10500 samples\n",
      "Epoch 1/20\n",
      "35000/35000 [==============================] - 7s 192us/step - loss: 0.6282 - acc: 0.6436 - val_loss: 0.4975 - val_acc: 0.8764\n",
      "Epoch 2/20\n",
      "35000/35000 [==============================] - 7s 189us/step - loss: 0.4974 - acc: 0.7912 - val_loss: 0.3487 - val_acc: 0.9017\n",
      "Epoch 3/20\n",
      "35000/35000 [==============================] - 6s 180us/step - loss: 0.4117 - acc: 0.8446 - val_loss: 0.2730 - val_acc: 0.9109\n",
      "Epoch 4/20\n",
      "35000/35000 [==============================] - 6s 179us/step - loss: 0.3637 - acc: 0.8677 - val_loss: 0.2389 - val_acc: 0.9124\n",
      "Epoch 5/20\n",
      "35000/35000 [==============================] - 6s 181us/step - loss: 0.3262 - acc: 0.8803 - val_loss: 0.2208 - val_acc: 0.9215\n",
      "Epoch 6/20\n",
      "35000/35000 [==============================] - 7s 190us/step - loss: 0.3019 - acc: 0.8919 - val_loss: 0.2130 - val_acc: 0.9229\n",
      "Epoch 7/20\n",
      "35000/35000 [==============================] - 6s 179us/step - loss: 0.2740 - acc: 0.9040 - val_loss: 0.2062 - val_acc: 0.9255\n",
      "Epoch 8/20\n",
      "35000/35000 [==============================] - 7s 190us/step - loss: 0.2587 - acc: 0.9097 - val_loss: 0.2046 - val_acc: 0.9262\n",
      "Epoch 9/20\n",
      "35000/35000 [==============================] - 7s 191us/step - loss: 0.2454 - acc: 0.9159 - val_loss: 0.2061 - val_acc: 0.9271\n",
      "Epoch 10/20\n",
      "35000/35000 [==============================] - 6s 178us/step - loss: 0.2317 - acc: 0.9217 - val_loss: 0.2061 - val_acc: 0.9293\n",
      "Epoch 11/20\n",
      "35000/35000 [==============================] - 6s 183us/step - loss: 0.2197 - acc: 0.9260 - val_loss: 0.2065 - val_acc: 0.9299\n",
      "Epoch 12/20\n",
      "35000/35000 [==============================] - 6s 178us/step - loss: 0.2103 - acc: 0.9291 - val_loss: 0.2158 - val_acc: 0.9290\n",
      "Epoch 13/20\n",
      "35000/35000 [==============================] - 6s 176us/step - loss: 0.2037 - acc: 0.9310 - val_loss: 0.2118 - val_acc: 0.9314\n",
      "Epoch 14/20\n",
      "35000/35000 [==============================] - 6s 177us/step - loss: 0.2005 - acc: 0.9327 - val_loss: 0.2165 - val_acc: 0.9327\n",
      "Epoch 15/20\n",
      "35000/35000 [==============================] - 6s 186us/step - loss: 0.1950 - acc: 0.9327 - val_loss: 0.2183 - val_acc: 0.9330\n",
      "Epoch 16/20\n",
      "35000/35000 [==============================] - 6s 175us/step - loss: 0.1824 - acc: 0.9387 - val_loss: 0.2266 - val_acc: 0.9333\n",
      "Epoch 17/20\n",
      "35000/35000 [==============================] - 6s 178us/step - loss: 0.1820 - acc: 0.9383 - val_loss: 0.2252 - val_acc: 0.9326\n",
      "Epoch 18/20\n",
      "35000/35000 [==============================] - 6s 178us/step - loss: 0.1828 - acc: 0.9370 - val_loss: 0.2324 - val_acc: 0.9355\n",
      "Epoch 19/20\n",
      "35000/35000 [==============================] - 6s 181us/step - loss: 0.1735 - acc: 0.9406 - val_loss: 0.2359 - val_acc: 0.9359\n",
      "Epoch 20/20\n",
      "35000/35000 [==============================] - 6s 177us/step - loss: 0.1708 - acc: 0.9431 - val_loss: 0.2342 - val_acc: 0.9346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe766158cc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenamiento del modelo con dropout\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20, batch_size=512,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[keras.callbacks.TensorBoard(log_dir='logs_dropout'),TrainValTensorBoard(write_graph=False,log_dir='logs_dropout_2')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500/4500 [==============================] - 0s 86us/step\n",
      "Test loss: 0.252373355812\n",
      "Test accuracy: 0.928222222169\n"
     ]
    }
   ],
   "source": [
    "# evaluacion del modelo con el conjunto de pruebas\n",
    "test_loss, test_acc = model.evaluate(x=x_test, y=y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  1. ...,  0.  0.  0.]]\n",
      "test: 92.0714%\n"
     ]
    }
   ],
   "source": [
    "# prueba del modelo con un comentario desconocido por el modelo\n",
    "review = \"\"\"Endgame consists almost entirely of the downtime scenes that were always secretly everyone's favorite parts of these movies anyway.\"\"\"\n",
    "review_vec = get_indices_from_review(review)\n",
    "vec = vectorize_sequences([review_vec], dim=30000)\n",
    "print(vec)\n",
    "res = np.squeeze(model.predict(vec))\n",
    "print('test: {:.4f}%'.format(res * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
